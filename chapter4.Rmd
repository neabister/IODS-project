---
output:
  html_document:
    css: styles.css
---

#Chapter 4: Classification and clustering {.tabset}

In this exercise I analyze `Boston`data set available in `MASS` package. 


###More information on Boston data set:  

<https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html>

##Loading MASS and Boston data set

`Boston` data set contains information from 506 housing observation (rows) with 14 different variables (columns).

```{r, message=FALSE}
library(MASS)
data('Boston')

str(Boston)
summary(Boston)


```

##Visualize and summarize data

Nice corrplot modification examples <https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html>

```{r, message=FALSE}
library(corrplot)
library(RColorBrewer)


correlations <- cor(Boston)
round(correlations, digits = 2)
colors <- brewer.pal(n = 9, name = "Pastel1")
signf_test <- cor.mtest(Boston, conf.level = .95)

corrplot(correlations, type = 'upper', method = 'ellipse', order = "hclust", col = brewer.pal(n = 8, name = "PiYG"), bg = colors[length(colors)], 
         p.mat = signf_test$p, insig = 'p-value', sig.level = .05, tl.col = "black", tl.srt = 90)

```

Above `corrplot` shows negative correlations in pink and positive correlations in green. Narrowness of `method = 'ellipse'` indicates how high correlation is. For non-significant correlations (p>0.05), p-values are shown.  

From the graph it can be observed that most of the variables correlate significantly with others. Only few pairs are not significantly correlated.

##Scale variables and categorical crimes

To be able to accurately classify the data, variable values need to be scaled so that all variables have a mean value of 0. It is done as follows (when all variables are numerical, as expected for classification analysis):

```{r}
scaled <- as.data.frame(scale(Boston))
class(scaled)
str(scaled)
summary(scaled)

```

Next, `crim` is cut to categorical variable according to quantiles to be able to later use it to train the model to predict the right crime rate class of an observation based on other variables.

```{r}
bins = quantile(scaled$crim)

crime <- cut(scaled$crim, breaks = bins, label = c('low', 'med_low', 'med_high', 'high'), include.lowest = TRUE)

#count table for each category level
table(crime)

#replace original crim with categorical crime variable

scaled <- dplyr::select(scaled, -crim)
scaled <- data.frame(scaled, crime)
head(scaled)

```

##Divide data for training and test sets

To be able to evaluate how well our model is predicting crime rate, I want to separate small fraction of the data (20%) for testing it, so it will not be used for training the model. Observations are selected randomly below for training or test sets.

```{r}
random_test_rows <- sample(nrow(scaled), size = nrow(scaled) * 0.2)

test_set <- scaled[random_test_rows, ]
train_set <- scaled[-random_test_rows, ]

#Check that resulting dfs are as should
dim(test_set)
dim(train_set)

```

##Fitting linear discriminant analysis for crimes

Fitting classification model with `lda()` function using `crimes` as a categorical variable and all other (continuous) variables as predicting variables.

```{r}
lda_fit <- lda(crime ~ ., data = train_set)
lda_fit

```

###Visualization of model

Using `ggsci` color palette and `ggord` package to visualize `lda_fit`. To install `ggord` package from Github, I use `install_github` function from `devtools` package.

```{r, message=FALSE, warning=FALSE}

#Convert crime factor levels to numeric to plot in different colors
crime_levels <- as.numeric(train_set$crime)

#I hate the default colors of the plot, so I'm using ggsci package palettes instead
#Good source for color palettes https://www.datanovia.com/en/blog/top-r-color-palettes-to-know-for-great-data-visualization/
library(ggsci)
library(devtools)
install_github("fawda123/ggord")
library(ggord)

cols <- pal_aaas()(4)


#Plot with nicer colors

ggord(lda_fit, train_set$crime, poly = FALSE, arrow=.3, veclsz = .5, vec_ext = 4, size=1, cols = cols)

```

There are so many variables in the model, that the arrows look a bit messy. However, it is easy to see still which variables affect the classification most (zn, rad, nox). This `ggord` package was very nice and easy to use. You can see that the model does not classify crime rates perfectly but I would say it does pretty good job distinguishing `high` crime rate from others in `train_set`.

##Test model

First need to create dataframe with correct crime classes for the test data and remove crime variable from test data that is used to predict the classes with the `lda_fit`.

```{r}
#Save correct classes to variable
correct_classes <- test_set$crime

#Remove classes from test data
test_set <- dplyr::select(test_set, -crime)

#Predict classes with model
lda_predict <- predict(lda_fit, newdata = test_set)

#Make 2X2 table to observe model accuracy

table(correct = correct_classes, predicted = lda_predict$class)

#Calculate percentage of right predictions on test data
percent_correct <- 100 * mean(lda_predict$class==correct_classes)
percent_correct <- round(percent_correct, digits = 0)

percent_correct

```

Above analysis of the model shows, that it predicted the crime class for `r percent_correct` % of `test_set` data correctly. Prediction accuracy is 100 % for high crime rate but less accurate for lower crime level classes. The worst accuracy is for med_low crime rate where almost half of the test data was classified wrong.

##K-means clustering

I am reading again `Boston` data set and scaling it for clustering by K-means.
First I am calculating Euclidean distances:

```{r}
data(Boston)
scaled_kmeans <- as.data.frame(scale(Boston))
eu_dist <- dist(scaled_kmeans)
summary(eu_dist)

```

Next I am running k-means clustering using defined seed:

```{r, message=FALSE}
library(ggplot2)
set.seed(123)

#Setting maximum number of clusters
max_seeds <- 10

#Finding optimal number of clusters with so called elbow method
twcss <- sapply(1:max_seeds, function(k) {kmeans(scaled_kmeans, k)$tot.withinss})
qplot(x = 1:max_seeds, y = twcss, geom = 'line')

```

I decided to use 3 clusters because `twcss` is still decreasing and I was not satisfied how 2 or more than 3 clusters looked like (I tested 2, 4 and 8 clusters).

```{r}
set.seed(123)
km <- kmeans(scaled_kmeans, centers = 3)

cols <- pal_futurama()(3)
cols_clusters <- cols[km$cluster]
pairs(scaled_kmeans, col = cols_clusters)

```

##Bonus: LDA using k-means clusters

Fitting `lda()` using k-means clusters as dependent variables and all variables in data set as explanatory variables. Data used is scaled Boston data.

```{r}
lda_kmeans <- lda(km$cluster ~ ., data = scaled_kmeans)
lda_kmeans

```

To visualize fitted model, I use again `ggord` function. For it to work, `km$cluster`s need to be converted to `factor()` because it can't be in numeric form for this function. Using same colors as before.

```{r}
cols <- pal_aaas()(3)

ggord(lda_kmeans, factor(km$cluster), poly = FALSE, arrow=.3, veclsz = .5, vec_ext = 4, size=1, cols = cols)

```

All 3 clusters can be separated quite nicely from each other, although only cluster 2 is clearly distinct from two others. Clustering is anyway better than clusters for crime rates as target classes. In this model, the most influential variables are `tax`, `rad` and `age`. However, it was clear that everytime k-means is executed, the clusters formed will be different making the interpretation and meaning of different clusters quite difficult.  

Below is shown LDA for k-means with 6 clusters and this shows `crim` and `black` as most influential variables but not all cluster separate nicely based on `LD1` and `LD2` that explain around 70 % of effect.

```{r}
#6 clusters

set.seed(123)
km6 <- kmeans(scaled_kmeans, centers = 6)

lda_kmeans6 <- lda(km6$cluster ~ ., data = scaled_kmeans)

cols <- pal_aaas()(6)

ggord(lda_kmeans6, factor(km6$cluster), poly = FALSE, arrow=.3, veclsz = .5, vec_ext = 4, size=1, cols = cols)

```

##Super-bonus

```{r, message=FALSE}
model_predictors <- dplyr::select(train_set, -crime)
# check the dimensions
dim(model_predictors)
dim(lda_fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda_fit$scaling
matrix_product <- as.data.frame(matrix_product)

#Next, install and access the plotly package. Create a 3D plot (Cool!) of the columns of the matrix product by typing the code below.
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train_set$crime)

```
