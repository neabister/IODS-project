# Chapter 2: Regression and model validation  

In this exercise I analyze preprocessed data.  

###Link to preprocessing R script:  
<https://github.com/neabister/IODS-project/blob/master/data/create_learning2014.R>
  
###Link to original data:  

<http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt>
  

###Original data and preprocessing  
Data is from Introduction to statistics course learning approaches study carried out in Helsinki 2014 and it is a part of multinational study. Approaches are defined by ASSIST guidelines and divided into three categories: Deep, Surface and Strategic study approaches. Students participated by filling in questionaires relating to these 3 approaches by answering 1-5 (least agreeing - most agreeing). In addition, data contains a measure for each student's attitude and achievements based on exam points.

In preprocessing, questions were divided into **Deep, Surface** and **Strategic** groups and for each group mean was calculated for each student. Analysis dataset includes additional information on gender, attitude and exam points. Students who had 0 points from exam were excluded.  

###Data analysis  

####Read the data and explore it  
```{r}
data <- read.csv('data/analysis_dataset.csv')
head(data)
dim(data)
str(data)

```
  
*Data contains 7 variables (columns) from 166 observations (students, rows). All variables are numeric or integers except gender which has 2 factors (female or male).*  

###Summary and graphical overview of the data  

```{r}
summary(data)
```

```{r}
library(ggplot2)
library(GGally)

p <- ggpairs(data, title = "Correlation summary graphs", mapping = aes(col = gender, alpha = 0.4), lower = list(combo = wrap('facethist', bins = 20)))

p
```

*Data contains more females than males but other variables are not dependent on gender. Participants age between 17-55. Gender independent **highest (positive) correlation is between points and attitude** (0.437) and **lowest (negative) correlation between points and deep learning approaches** (-0.0101). This could suggest that exams do not measure level of deep learning. However, points correlate best with strategic approaches (positive correlation), suggesting that focus on planning and scheduling could improve exam points.*

###Linear regression fitting  

To try to explain target variable **points**, I selected 3 variables that had highest negative or positive correlation (**attitude**, **stra**, **surf**) with it and fitted linear regression.  

```{r}
linear_3_variables <- lm(points ~ attitude + stra + surf, data = data)
summary(linear_3_variables)
```

*Attitude is the only significant variable to explain points.*  
Fitting linear regression using only significant **attitude** variable:  

```{r}
linear_attitude <- lm(points ~ attitude, data = data)
summary(linear_attitude)
```

*By removing non-significant variables, Adj. R^2^ is marginally reducing, meaning that slightly less of the variance in **points** is explained by **attitude**. However, F-statistic p-value is also decreasing in a model with attitude variable alone, suggesting that the fit of a single regression model would be better for the data. Both of the models give relatively high residual std errors when compared to 1st and 3rd residual quantiles *  

###Diagnostic plots for multiple regression  

`Residuals vs Fitted values`, `Normal QQ-plot and` and `Residuals vs Leverage` diagnostic plots produced.  

```{r}

plot_diagnostics2 <- plot(linear_attitude, which = c(1, 2, 5), par(mfrow = c(1,2)))

```

***Residuals vs. Fitted values**: Reasonable constant variation of residual errors, graph should not show any patterns.*  
***QQ-plot:** Normality of errors is reasonable*  
***Leverage:** Indicates the impact of individual points on the fitted model. My model has reasonable leverage.*

###Testing stepwise multiple regression

I realized it would be slow to observe manually all possible variable combinations, so I looked for an automated one to ensure I find the best model. Here I test `ols_step_both_p` function from  `olsrr package`. It is selecting variables to the model based on their p values and the thresholds to include or exclude a variable can be manually adjusted (more information here <https://www.guru99.com/r-simple-multiple-linear-regression.html>.  

```{r}
library(olsrr)

fit <- lm(points ~ factor(gender) + age + attitude + deep + stra + surf, data = data) 
best <- ols_step_both_p(fit)
best

```

###Best fit diagnostic plots

```{r}
best_fit <- lm(points ~ attitude + stra + age, data = data)
summary(best_fit)
best_fit_diagn <- plot(best_fit, which = c(1, 2, 5), par(mfrow = c(1,2)))

```

###Conclusions

Overall power of the fitted models to explain points variablity is limited. Adjusted R^2^ is quite low for both models and residual errors remain quite high. Based on F-statistics p-value, and observation that attitude was the only significant variable associated with points, I propose that single regression model would be marginally better model to use for predicting student points and would give reasonable predictions while being simple. The better the attitude of a student is, the better exam points they are likely to have.
